---
layout: post
title:  "Bias Variance Trade Off: Variance Estimation with Bootstrapping!"
date:   2019-10-07 9:56:15 
---

<h2>1 John 4:18</h2>


<em>There is no fear in love. But perfect love drives out fear, because fear has to do with punishment. The one who fears is not made perfect in love </em>





Bootstrapping is used to estimate the variance of a model. It tells you how different is a model when you run it multiple times.It consists of constructing a number artificial datasets drawn with replacement from an original dataset. these datasets are then trained on a specific model. Please note that each dataset created has a distribution similar to the original one.

**Bagging** (or Bootstrap Aggregating) is an application of bootstrapping. This is an algorithm that helps reduce variance. Multiple classifiers are trained and a strong learner is finally derived by averaging the weak learners (classifiers).

<h2>Advantages of Bagging</h2>

<p>
<ol>Bagging has the following advantages:
    
<li>Parallelism: you can training multiple models at the same time in parallel depending on your computer power.</li>

<li>Cross Validation: Bagging uses the concept of cross validation in the sense that for each random sample, a train and test datasets are created.</li>

</ol>
</p>

**Random Forest** is a variation of cart trees that uses the bagging methodology. Random forests are very appealing in the sense they have no bias, the only focus now being the improvement of the variance. Random forest are trees that are quick to train using subsamples of k dimensions.

<h2>Random Forest</h2>

<p>
<ol>Random Forest has the following advantages:
    
    <li>No Scaling required: No need to scale the features as we are only dealing with data splitting through the cart method.</li>

    <li>Linear or non linear distributions: It doesn't matter what kind of distribution you are using because of the beauty of resampling.</li>

    <li>Easy to tune the hyperparameters: There are only two parameters involved: the numbers of trees and k, the number of dimensions to use for each iteration. By the rule of thumb, k is the square root of the total number of features.</li>
</ol>
</p>

<h2>Let's create bagging out of scratch in python</h2>

Each tree is built using n random samples of training data with replacement. All of the features are represented here. Let us just return a list of trees as output for now.


```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
import matplotlib.pyplot as plt
```


```python
def bagging(x, y, m, depth=np.inf):
    """Creates a bagging function.
    
    In:
        x:      (n x d) matrix of data points
        y:      n-dimensional vector of labels
        m:        number of trees in the bagging
        depth: maximum depth of tree
        
    Out:
        tree_list: list of decision trees of length m
    """
    
    n, d = x.shape
    tree_list = []
    for iTrees in range(m):
        idxBag = np.random.choice(x.shape[0],x.shape[0],replace = True)    
        xTrainBag = np.array([x[i] for i in idxBag])  
        yTrainBag = np.array([y[i] for i in idxBag])
        tree = DecisionTreeRegressor(max_depth=depth)
        tree.fit(xTrainBag,yTrainBag)
        tree_list.append(tree)
    return tree_list
```

Since there are 20 samples, we should expect 20 trees. 


```python
m = 20
x = np.arange(1000).reshape((1000, 1))
y = np.arange(1000)
depth = 5
trees_collected = bagging(x, y, m, depth)
```


```python
print (len(trees_collected))
```

    20
    

Let's now evaluate the bagging by taking the average prediction of all the trees collected


```python
def baggingOut(trees, x):
    """Evaluates x using trees.
    
    In:
        trees:  list of decision trees of length m
        x:      n x d matrix of data points
        
    Out:
        pred: n-dimensional vector of predictions
    """
    m = len(trees)
    n,d = x.shape
    
    predictions = np.zeros(n)
    
    predictions = []
    for i in range(len(trees)):
        preds = trees[i].predict(x)
        predictions.append(preds)

    predictions = np.mean(predictions, axis = 0)  
    return predictions
```


```python
xtest = np.arange(100).reshape((100, 1))
preds = baggingOut(trees_collected, xtest)
```


```python
preds[:5]
```




    array([13.98825896, 13.98825896, 13.98825896, 13.98825896, 13.98825896])



Please check out this website also if interested in <a href="https://www.realestatedipdive.com/locations/florida/" rel="nofollow">Land Investment</a>


```python

```
